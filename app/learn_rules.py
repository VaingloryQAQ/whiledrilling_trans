"""è§„åˆ™å­¦ä¹ å‘½ä»¤è¡Œå·¥å…·"""

import argparse
import sys
from pathlib import Path
import json

from .rule_learner import RuleLearner
from .ml_classifier import MLClassifier, HybridClassifier

def main():
    parser = argparse.ArgumentParser(description='ä»CSVæ•°æ®å­¦ä¹ åˆ†ç±»è§„åˆ™')
    parser.add_argument('--csv-dir', type=Path, default=Path('data/api_csv'),
                       help='CSVæ–‡ä»¶ç›®å½• (é»˜è®¤: data/api_csv)')
    parser.add_argument('--output-dir', type=Path, default=Path('data/learned_rules'),
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: data/learned_rules)')
    parser.add_argument('--train-ml', action='store_true',
                       help='è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹')
    parser.add_argument('--ml-model-path', type=Path, default=Path('data/models/ml_classifier.pkl'),
                       help='MLæ¨¡å‹ä¿å­˜è·¯å¾„ (é»˜è®¤: data/models/ml_classifier.pkl)')
    parser.add_argument('--evaluate', action='store_true',
                       help='è¯„ä¼°å­¦ä¹ åˆ°çš„è§„åˆ™')
    parser.add_argument('--apply-enhanced-rules', action='store_true',
                       help='åº”ç”¨å¢å¼ºè§„åˆ™åˆ°ç°æœ‰ç³»ç»Ÿ')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥CSVç›®å½•æ˜¯å¦å­˜åœ¨
    if not args.csv_dir.exists():
        print(f"é”™è¯¯: CSVç›®å½•ä¸å­˜åœ¨: {args.csv_dir}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=== é’»äº•å›¾åƒåˆ†ç±»è§„åˆ™å­¦ä¹ å·¥å…· ===")
    print(f"CSVç›®å½•: {args.csv_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print()
    
    # 1. å­¦ä¹ è§„åˆ™
    print("1. å¼€å§‹å­¦ä¹ è§„åˆ™...")
    learner = RuleLearner(args.csv_dir, args.output_dir)
    learning_result = learner.learn_from_csv_data()
    
    print(f"âœ“ è§„åˆ™å­¦ä¹ å®Œæˆ")
    print(f"  - åˆ†æäº† {len(learning_result['analysis']['analysis_stats'])} ä¸ªCSVæ–‡ä»¶")
    print(f"  - ç”Ÿæˆäº† {len(learning_result['classification_rules']['categories'])} ä¸ªåˆ†ç±»è§„åˆ™")
    print(f"  - å‘ç°äº† {len(learning_result['patterns']['special_tokens'])} ä¸ªç‰¹æ®Šæ ‡è®°")
    print()
    
    # 2. è®­ç»ƒMLæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    if args.train_ml:
        print("2. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        ml_model_path = args.ml_model_path
        ml_model_path.parent.mkdir(parents=True, exist_ok=True)
        
        ml_classifier = MLClassifier(ml_model_path)
        training_result = ml_classifier.train(args.csv_dir)
        
        print(f"âœ“ MLæ¨¡å‹è®­ç»ƒå®Œæˆ")
        print(f"  - å‡†ç¡®ç‡: {training_result['accuracy']:.3f}")
        print(f"  - è®­ç»ƒæ ·æœ¬: {training_result['training_samples']}")
        print(f"  - æµ‹è¯•æ ·æœ¬: {training_result['test_samples']}")
        print(f"  - ç‰¹å¾æ•°é‡: {training_result['feature_count']}")
        print()
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        with open(args.output_dir / 'ml_training_result.json', 'w', encoding='utf-8') as f:
            json.dump(training_result, f, ensure_ascii=False, indent=2)
    
    # 3. è¯„ä¼°è§„åˆ™ï¼ˆå¯é€‰ï¼‰
    if args.evaluate:
        print("3. è¯„ä¼°å­¦ä¹ åˆ°çš„è§„åˆ™...")
        evaluation_result = learner.evaluate_learned_rules()
        
        print(f"âœ“ è§„åˆ™è¯„ä¼°å®Œæˆ")
        print(f"  - æµ‹è¯•æ–‡ä»¶æ•°: {evaluation_result['test_files_count']}")
        print(f"  - è§„åˆ™å¼•æ“æˆåŠŸç‡: {evaluation_result['performance']['rule_based']['success_rate']:.3f}")
        if 'ml_based' in evaluation_result['performance']:
            print(f"  - MLæ¨¡å‹æˆåŠŸç‡: {evaluation_result['performance']['ml_based']['success_rate']:.3f}")
        print()
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        with open(args.output_dir / 'evaluation_result.json', 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
    
    # 4. åº”ç”¨å¢å¼ºè§„åˆ™ï¼ˆå¯é€‰ï¼‰
    if args.apply_enhanced_rules:
        print("4. åº”ç”¨å¢å¼ºè§„åˆ™...")
        enhanced_rules_path = args.output_dir / 'enhanced_rules.yaml'
        
        if enhanced_rules_path.exists():
            # å¤‡ä»½ç°æœ‰è§„åˆ™
            current_rules_path = Path('config/rules.yaml')
            if current_rules_path.exists():
                backup_path = Path('config/rules.yaml.backup')
                import shutil
                shutil.copy2(current_rules_path, backup_path)
                print(f"  - å·²å¤‡ä»½ç°æœ‰è§„åˆ™åˆ°: {backup_path}")
            
            # åº”ç”¨æ–°è§„åˆ™
            import shutil
            shutil.copy2(enhanced_rules_path, current_rules_path)
            print(f"  - å·²åº”ç”¨å¢å¼ºè§„åˆ™åˆ°: {current_rules_path}")
        else:
            print(f"  - é”™è¯¯: å¢å¼ºè§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {enhanced_rules_path}")
        print()
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    print("5. ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š...")
    generate_learning_report(learning_result, args.output_dir)
    print(f"âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ: {args.output_dir / 'learning_report.md'}")
    
    print("\n=== è§„åˆ™å­¦ä¹ å®Œæˆ ===")
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
    
    # æ˜¾ç¤ºå…³é”®å»ºè®®
    if learning_result['suggestions']:
        print("\nå…³é”®å»ºè®®:")
        for suggestion in learning_result['suggestions']:
            if suggestion['priority'] == 'high':
                print(f"  ğŸ”´ {suggestion['message']}")
            elif suggestion['priority'] == 'medium':
                print(f"  ğŸŸ¡ {suggestion['message']}")
            else:
                print(f"  ğŸŸ¢ {suggestion['message']}")


def generate_learning_report(learning_result: dict, output_dir: Path):
    """ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š"""
    report_content = """# é’»äº•å›¾åƒåˆ†ç±»è§„åˆ™å­¦ä¹ æŠ¥å‘Š

## æ¦‚è¿°

æœ¬æŠ¥å‘ŠåŸºäºCSVæ•°æ®åˆ†æç»“æœï¼Œè‡ªåŠ¨å­¦ä¹ äº†é’»äº•å›¾åƒåˆ†ç±»è§„åˆ™ã€‚

## æ•°æ®ç»Ÿè®¡

### CSVæ–‡ä»¶åˆ†æ
"""
    
    # æ·»åŠ CSVåˆ†æç»Ÿè®¡
    for api_source, stats in learning_result['analysis']['analysis_stats'].items():
        report_content += f"""
#### {api_source}
- æ€»æ–‡ä»¶æ•°: {stats['total_files']}
- å”¯ä¸€äº•åæ•°: {stats['unique_wells']}
- æ ·å“ç±»å‹åˆ†å¸ƒ: {stats['sample_type_distribution']}
- ç±»åˆ«åˆ†å¸ƒ: {stats['category_distribution']}
"""
    
    # æ·»åŠ æ¨¡å¼åˆ†æ
    patterns = learning_result['patterns']
    report_content += f"""
## æ¨¡å¼åˆ†æ

### äº•åæ¨¡å¼
å‘ç° {len(patterns['well_patterns'])} ç§äº•åæ¨¡å¼

### æ·±åº¦æ¨¡å¼
å‘ç° {len(patterns['depth_patterns'])} ç§æ·±åº¦æ¨¡å¼

### ç‰¹æ®Šæ ‡è®°
å‘ç° {len(patterns['special_tokens'])} ç§ç‰¹æ®Šæ ‡è®°: {', '.join(patterns['special_tokens'])}

## å­¦ä¹ åˆ°çš„è§„åˆ™

### åˆ†ç±»è§„åˆ™
"""
    
    for rule in learning_result['classification_rules']['categories']:
        report_content += f"""
#### {rule['name']}
- æ¨¡å¼: {rule['mode']}
- ä¼˜å…ˆçº§: {rule['priority']}
- å¿…éœ€æ¡ä»¶: {rule['require_all']}
- ç½®ä¿¡åº¦: {rule.get('confidence', 0):.3f}
"""
    
    # æ·»åŠ å»ºè®®
    if learning_result['suggestions']:
        report_content += """
## è§„åˆ™å»ºè®®

"""
        for suggestion in learning_result['suggestions']:
            priority_icon = {
                'high': 'ğŸ”´',
                'medium': 'ğŸŸ¡',
                'low': 'ğŸŸ¢'
            }.get(suggestion['priority'], 'âšª')
            
            report_content += f"- {priority_icon} **{suggestion['type']}**: {suggestion['message']}\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_dir / 'learning_report.md', 'w', encoding='utf-8') as f:
        f.write(report_content)


if __name__ == '__main__':
    main()
