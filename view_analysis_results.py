#!/usr/bin/env python3
"""æŸ¥çœ‹è¯¦ç»†åˆ†æžç»“æžœ"""

import json
import sys
from pathlib import Path

def view_analysis_summary(analysis_dir: Path):
    """æŸ¥çœ‹åˆ†æžç»“æžœæ‘˜è¦"""
    print("=== é’»äº•å›¾åƒæ•°æ®è¯¦ç»†åˆ†æžç»“æžœ ===")
    print(f"åˆ†æžç›®å½•: {analysis_dir}")
    print()
    
    # 1. äº•ååˆ†æž
    well_file = analysis_dir / 'well_pattern_analysis.json'
    if well_file.exists():
        with open(well_file, 'r', encoding='utf-8') as f:
            well_data = json.load(f)
        
        print("ðŸ“Š äº•ååˆ†æž:")
        print(f"  æ€»äº•æ•°: {well_data['total_wells']}")
        print(f"  äº•åå‰ç¼€æ•°: {well_data['well_statistics']['prefix_count']}")
        print(f"  æœ€å¸¸è§å‰ç¼€: {well_data['well_statistics']['most_common_prefix'][0]} ({well_data['well_statistics']['most_common_prefix'][1]}ä¸ª)")
        print(f"  å¹³å‡æ¯å‰ç¼€äº•æ•°: {well_data['well_statistics']['avg_wells_per_prefix']:.1f}")
        
        print("  äº•åå‰ç¼€åˆ†å¸ƒ:")
        for prefix, count in sorted(well_data['well_categories'].items(), key=lambda x: x[1], reverse=True):
            print(f"    {prefix}: {count}ä¸ª")
        print()
    
    # 2. æ·±åº¦åˆ†æž
    depth_file = analysis_dir / 'depth_pattern_analysis.json'
    if depth_file.exists():
        with open(depth_file, 'r', encoding='utf-8') as f:
            depth_data = json.load(f)
        
        print("ðŸ“Š æ·±åº¦æ¨¡å¼åˆ†æž:")
        print(f"  æ€»æ·±åº¦æ¨¡å¼æ•°: {depth_data['total_patterns']}")
        
        pattern_types = depth_data['pattern_types']
        print(f"  å•æ·±åº¦æ¨¡å¼: {pattern_types['single_depth']['count']}ä¸ª")
        print(f"  æ·±åº¦åŒºé—´æ¨¡å¼: {pattern_types['range_depth']['count']}ä¸ª")
        print(f"  å…¶ä»–æ¨¡å¼: {pattern_types['other_patterns']['count']}ä¸ª")
        
        if 'depth_statistics' in depth_data:
            stats = depth_data['depth_statistics']
            print(f"  æ·±åº¦èŒƒå›´: {stats['min_depth']:.1f}m - {stats['max_depth']:.1f}m")
            print(f"  å¹³å‡æ·±åº¦: {stats['avg_depth']:.1f}m")
            
            dist = stats['depth_distribution']
            print("  æ·±åº¦åˆ†å¸ƒ:")
            print(f"    æµ…å±‚(0-1000m): {dist['shallow_0_1000']}ä¸ª")
            print(f"    ä¸­å±‚(1000-3000m): {dist['medium_1000_3000']}ä¸ª")
            print(f"    æ·±å±‚(3000-5000m): {dist['deep_3000_5000']}ä¸ª")
            print(f"    è¶…æ·±å±‚(5000m+): {dist['very_deep_5000+']}ä¸ª")
        print()
    
    # 3. å¼‚å¸¸åˆ†æž
    anomaly_file = analysis_dir / 'anomaly_analysis.json'
    if anomaly_file.exists():
        with open(anomaly_file, 'r', encoding='utf-8') as f:
            anomaly_data = json.load(f)
        
        print("ðŸš¨ å¼‚å¸¸åˆ†æž:")
        print(f"  æ€»å¼‚å¸¸æ•°: {anomaly_data['total_anomalies']}")
        
        print("  å¼‚å¸¸ç±»åž‹åˆ†å¸ƒ:")
        for anomaly_type, count in sorted(anomaly_data['anomaly_types'].items(), key=lambda x: x[1], reverse=True):
            print(f"    {anomaly_type}: {count}ä¸ª")
        
        if anomaly_data['anomalies']:
            print("  å‰5ä¸ªä¸¥é‡å¼‚å¸¸:")
            for i, anomaly in enumerate(anomaly_data['anomalies'][:5]):
                print(f"    {i+1}. {anomaly['filename']}")
                print(f"       å¼‚å¸¸: {', '.join(anomaly['anomalies'])} (ä¸¥é‡åº¦: {anomaly['severity']})")
        print()
    
    # 4. æ–‡ä»¶æ‰©å±•ååˆ†æž
    ext_file = analysis_dir / 'extension_analysis.json'
    if ext_file.exists():
        with open(ext_file, 'r', encoding='utf-8') as f:
            ext_data = json.load(f)
        
        print("ðŸ“ æ–‡ä»¶æ‰©å±•ååˆ†æž:")
        print(f"  æ€»æ‰©å±•åç±»åž‹: {ext_data['total_extensions']}")
        print("  æ‰©å±•ååˆ†å¸ƒ:")
        for ext, count in sorted(ext_data['extension_statistics'].items(), key=lambda x: x[1], reverse=True):
            print(f"    .{ext}: {count}ä¸ª")
        print()
    
    # 5. ç‰¹æ®Šæ ‡è®°åˆ†æž
    token_file = analysis_dir / 'special_token_analysis.json'
    if token_file.exists():
        with open(token_file, 'r', encoding='utf-8') as f:
            token_data = json.load(f)
        
        print("ðŸ·ï¸  ç‰¹æ®Šæ ‡è®°åˆ†æž:")
        print(f"  æ€»ç‰¹æ®Šæ ‡è®°æ•°: {token_data['total_tokens']}")
        
        for category, tokens in token_data['token_categories'].items():
            print(f"  {category}: {', '.join(tokens)}")
        print()
    
    # 6. åˆ†ç±»è§„åˆ™åˆ†æž
    class_file = analysis_dir / 'classification_analysis.json'
    if class_file.exists():
        with open(class_file, 'r', encoding='utf-8') as f:
            class_data = json.load(f)
        
        print("ðŸ” åˆ†ç±»è§„åˆ™åˆ†æž:")
        print(f"  æ€»è§„åˆ™æ•°: {class_data['total_rules']}")
        
        print("  æŒ‰ä¼˜å…ˆçº§åˆ†ç»„:")
        for priority in sorted(class_data['rules_by_priority'].keys(), reverse=True):
            rules = class_data['rules_by_priority'][priority]
            print(f"    ä¼˜å…ˆçº§{priority}: {', '.join(rules)}")
        
        print("  æŒ‰ç½®ä¿¡åº¦åˆ†ç»„:")
        for confidence_range, rules in class_data['rules_by_confidence'].items():
            print(f"    {confidence_range}: {', '.join(rules)}")
        print()

def view_specific_analysis(analysis_dir: Path, analysis_type: str):
    """æŸ¥çœ‹ç‰¹å®šç±»åž‹çš„è¯¦ç»†åˆ†æž"""
    print(f"=== {analysis_type} è¯¦ç»†åˆ†æž ===")
    
    if analysis_type == 'wells':
        # æŸ¥çœ‹äº•ååˆ—è¡¨
        well_file = analysis_dir / 'well_names.txt'
        if well_file.exists():
            with open(well_file, 'r', encoding='utf-8') as f:
                wells = f.readlines()
            
            print(f"æ€»äº•æ•°: {len(wells)}")
            print("å‰20ä¸ªäº•å:")
            for i, well in enumerate(wells[:20]):
                print(f"  {i+1:2d}. {well.strip()}")
            
            if len(wells) > 20:
                print(f"  ... è¿˜æœ‰ {len(wells) - 20} ä¸ªäº•å")
    
    elif analysis_type == 'depths':
        # æŸ¥çœ‹æ·±åº¦æ¨¡å¼
        depth_file = analysis_dir / 'depth_patterns.txt'
        if depth_file.exists():
            with open(depth_file, 'r', encoding='utf-8') as f:
                depths = f.readlines()
            
            print(f"æ€»æ·±åº¦æ¨¡å¼æ•°: {len(depths)}")
            print("å‰20ä¸ªæ·±åº¦æ¨¡å¼:")
            for i, depth in enumerate(depths[:20]):
                print(f"  {i+1:2d}. {depth.strip()}")
            
            if len(depths) > 20:
                print(f"  ... è¿˜æœ‰ {len(depths) - 20} ä¸ªæ·±åº¦æ¨¡å¼")
    
    elif analysis_type == 'anomalies':
        # æŸ¥çœ‹å¼‚å¸¸æ–‡ä»¶å
        anomaly_file = analysis_dir / 'anomaly_filenames.txt'
        if anomaly_file.exists():
            with open(anomaly_file, 'r', encoding='utf-8') as f:
                anomalies = f.readlines()
            
            print(f"æ€»å¼‚å¸¸æ•°: {len(anomalies)}")
            print("å‰20ä¸ªå¼‚å¸¸æ–‡ä»¶å:")
            for i, anomaly in enumerate(anomalies[:20]):
                print(f"  {i+1:2d}. {anomaly.strip()}")
            
            if len(anomalies) > 20:
                print(f"  ... è¿˜æœ‰ {len(anomalies) - 20} ä¸ªå¼‚å¸¸")

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python view_analysis_results.py <åˆ†æžç›®å½•> [åˆ†æžç±»åž‹]")
        print("åˆ†æžç±»åž‹: summary, wells, depths, anomalies")
        sys.exit(1)
    
    analysis_dir = Path(sys.argv[1])
    if not analysis_dir.exists():
        print(f"é”™è¯¯: åˆ†æžç›®å½•ä¸å­˜åœ¨: {analysis_dir}")
        sys.exit(1)
    
    if len(sys.argv) > 2:
        analysis_type = sys.argv[2]
        if analysis_type == 'summary':
            view_analysis_summary(analysis_dir)
        else:
            view_specific_analysis(analysis_dir, analysis_type)
    else:
        view_analysis_summary(analysis_dir)

if __name__ == '__main__':
    main()
